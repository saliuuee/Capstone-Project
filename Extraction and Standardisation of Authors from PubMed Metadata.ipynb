{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymed import PubMed\n",
    "from string import punctuation\n",
    "import copy\n",
    "import datetime\n",
    "import pandas as pd\n",
    "from unidecode import unidecode\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from fuzzywuzzy import fuzz\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PubMed Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell to limit the number of articles extracted from the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MaxResults= 2000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main code for extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished extraction my g. 2000 articles. 18375 records\n"
     ]
    }
   ],
   "source": [
    "\n",
    "SearchTerms = 'Acute Myeloid Leukemia'\n",
    "\n",
    "\n",
    "\n",
    "pubmed = PubMed(tool=\"MyTool\", email=\"saliugiwaosagie@gmail.com\")\n",
    "\n",
    "results = pubmed.query(SearchTerms, max_results=MaxResults)\n",
    "\n",
    "\n",
    "\n",
    "results_dict = {}\n",
    "\n",
    "count = 0\n",
    "for article in results:\n",
    "    count +=1\n",
    "    results_dict.update({article.pubmed_id: {}})\n",
    "    results_dict[article.pubmed_id]['PMID'] = article.pubmed_id[:9].strip()\n",
    "    results_dict[article.pubmed_id]['title'] = article.title\n",
    "    #results_dict[article.pubmed_id]['abstract'] = article.abstract\n",
    "    results_dict[article.pubmed_id]['authors'] = article.authors\n",
    "    \n",
    "author_extraction = []\n",
    "extraction_publication_dict = {}\n",
    "\n",
    "prefix = 'extract_'\n",
    "\n",
    "x = 0\n",
    "\n",
    "for article in results_dict:\n",
    "    extraction_publication_dict[results_dict[article]['PMID']] = results_dict[article]['title']\n",
    "    \n",
    "    for author in results_dict[article]['authors']:\n",
    "        if author['lastname'] != None:\n",
    "            x +=1\n",
    "            current = []\n",
    "            first_name_split = None\n",
    "\n",
    "            current.append(prefix + f'{x:05}')\n",
    "            if author['firstname'] != None:\n",
    "                first_name_split = author['firstname'].split(' ')\n",
    "                if len(first_name_split) == 1:\n",
    "                    current.append(first_name_split[0])\n",
    "                    current.append('')\n",
    "                else:\n",
    "                    current.append(first_name_split[0])\n",
    "                    current.append(' '.join(first_name_split[1:]))\n",
    "            else:\n",
    "                current.append('')\n",
    "                current.append('')\n",
    "            current.append(author['lastname'])\n",
    "            if author['firstname'] != None:\n",
    "                current.append(' '.join([author['firstname'], author['lastname']]))\n",
    "            else:\n",
    "                current.append(author['lastname'])\n",
    "            if 'affiliation' in author and author['affiliation'] != None:\n",
    "                current.append(author['affiliation'])\n",
    "            else:\n",
    "                current.append('')\n",
    "            current.append(results_dict[article]['PMID'][:9].strip())\n",
    "            author_extraction.append(current)\n",
    "\n",
    "print(f'Finished extraction. {count} articles. {x} records') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PubMed extraction export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_list = [['ID', 'First Name', 'Middle Name', 'Last Name', 'Full Name', 'Affiliation', 'Activity ID' ]] + author_extraction\n",
    "with open('Python Pubmed Extraction.csv','w', newline='', encoding = \"UTF-8\") as dunno:\n",
    "    writer = csv.writer(dunno)\n",
    "    writer.writerows(results_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining functions used in tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test for initials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initial_tester(name_1, name_2):\n",
    "    \n",
    "    #Setting variables\n",
    "    match = False\n",
    "    first_single_letters = False\n",
    "    middle_single_letters = False\n",
    "    first_present = False\n",
    "    middle_present = False\n",
    "    short_name = None\n",
    "    long_name = None\n",
    "    \n",
    "    first_length_1 = len(name_1[0])\n",
    "    first_length_2 = len(name_2[0])\n",
    "    \n",
    "    middle_length_1 = len(name_1[1])\n",
    "    middle_length_2 = len(name_2[1])\n",
    "    \n",
    "       \n",
    "    last_name_1 = name_1[2]\n",
    "    last_name_2 = name_2[2]\n",
    "    \n",
    "    two_letter_and_hyphen = False\n",
    "    \n",
    "    #Test to see if first name with hyphens could be the same. Common in Korean names. e.g.,  s-w vs see-woo\n",
    "    if '-' in name_1[0] and '-' in name_2[0]:\n",
    "        if len(name_1[0]) == 3 and len(name_2[0]) > 3:\n",
    "            short_name = name_1\n",
    "            long_name = name_2\n",
    "        elif len(name_2[0]) == 3 and len(name_1[0]) > 3:\n",
    "            short_name = name_2\n",
    "            long_name = name_1\n",
    "        \n",
    "            \n",
    "    if (short_name != None\n",
    "        and short_name[0][0] == long_name[0][0]\n",
    "        and '-' + str(short_name[0][2]) in long_name[0]\n",
    "        and name_1[2] == name_2[2]):\n",
    "        \n",
    "        match = True\n",
    "        \n",
    "   #Test to see if two letter initals have been used in place of first name with hypen. e.g, JP  and Jean-Paul\n",
    "    if '-' in name_1[0] and '-' not in name_2[0] and len(name_2[0]) == 2:\n",
    "        hyphenated_first_name = name_1[0]\n",
    "        non_hyphenated_first_name = name_2[0]\n",
    "        two_letter_and_hyphen = True\n",
    "    elif '-' in name_2[0] and '-' not in name_1[0] and len(name_1[0]) ==2:\n",
    "        hyphenated_first_name = name_2[0]\n",
    "        non_hyphenated_first_name = name_1[0]\n",
    "        two_letter_and_hyphen = True\n",
    "        \n",
    "    if two_letter_and_hyphen == True:\n",
    "        if (non_hyphenated_first_name[0] == hyphenated_first_name[0]\n",
    "            and '-' + str(non_hyphenated_first_name[1]) in hyphenated_first_name\n",
    "            and name_1[2] == name_2[2]):\n",
    "            match = True\n",
    "\n",
    "\n",
    "    #Test to check if initials between names are the same\n",
    "    if (first_length_1 > 0\n",
    "        and first_length_2 > 0):\n",
    "        first_present = True\n",
    "        first_initial_1 = name_1[0][0]\n",
    "        first_initial_2 = name_2[0][0]\n",
    "    \n",
    "    if (middle_length_1 > 0\n",
    "        and\n",
    "        middle_length_2 > 0):\n",
    "        middle_present = True\n",
    "        middle_initial_1 = name_1[1][0]\n",
    "        middle_initial_2 = name_2[1][0]  \n",
    "        if (middle_length_1 == 1\n",
    "            or\n",
    "            middle_length_2 == 1):\n",
    "            middle_single_letters = True  \n",
    "    \n",
    "    if (first_length_1 <= 1\n",
    "        or\n",
    "        first_length_2 <= 1):\n",
    "        first_single_letters = True\n",
    "        \n",
    "    if first_present == True:\n",
    "        if middle_present == True:\n",
    "            if first_single_letters == True and middle_single_letters == True:\n",
    "                if (first_initial_1 == first_initial_2\n",
    "                    and\n",
    "                    middle_initial_1 == middle_initial_2\n",
    "                    and\n",
    "                    last_name_1 == last_name_2):\n",
    "                    match = True\n",
    "        else:\n",
    "            if first_single_letters == True:\n",
    "                if (first_initial_1 == first_initial_2\n",
    "                    and\n",
    "                    last_name_1 == last_name_2):            \n",
    "                    match = True\n",
    "        \n",
    "    \n",
    "    return match"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test for hyhenated names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyphen_tester(name_1, name_2):\n",
    "    prename_there = False\n",
    "    other_names_there = False\n",
    "    other_name_part = []\n",
    "    concat_name_1 = ' '.join(name_1).strip()\n",
    "    concat_name_2 = ' '.join(name_2).strip()\n",
    "    if ('-' in concat_name_1 and '-' in concat_name_2) or ('-' not in concat_name_1 and '-' not in concat_name_2):\n",
    "        return False\n",
    "    else:        \n",
    "        if '-' in concat_name_1:\n",
    "            hyphenated_name = name_1\n",
    "            non_hyphenated_name = name_2\n",
    "        elif '-' in concat_name_2:\n",
    "            hyphenated_name = name_2\n",
    "            non_hyphenated_name = name_1\n",
    "        \n",
    "        position = -1\n",
    "        for name in hyphenated_name:\n",
    "            position +=1\n",
    "            if '-' in name:\n",
    "                hyphenated_name_part = name\n",
    "                final_position = position\n",
    "            else:\n",
    "                other_name_part.append(name)\n",
    "            \n",
    "        prename = hyphenated_name_part[:hyphenated_name_part.find('-')]\n",
    "        \n",
    "        position_2 = -1\n",
    "        for name in non_hyphenated_name:\n",
    "            position_2 +=1\n",
    "            if prename == name:\n",
    "                prename_there = True\n",
    "                final_position_2 = position_2\n",
    "        \n",
    "        other_name_position_1 = 0\n",
    "        other_name_position_2 = 0\n",
    "        for name in other_name_part:\n",
    "            other_name_position_1 += 1\n",
    "            for name2 in non_hyphenated_name:\n",
    "                other_name_position_2 += 1\n",
    "                if name == name2 and name != '' and len(name) > 1 and other_name_position_1 == other_name_position_2:\n",
    "                    other_names_there = True\n",
    "            \n",
    "    \n",
    "        if other_names_there == True and prename_there == True and final_position == final_position_2 and len(prename) > 1:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function for testing common words between affilaitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = [\"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\", \"herself\", \"it\", \"its\", \"itself\", \"they\", \"them\", \"their\", \"theirs\", \"themselves\", \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\", \"these\", \"those\", \"am\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"having\", \"do\", \"does\", \"did\", \"doing\", \"a\", \"an\", 'the', \"and\", \"but\", \"if\", \"or\", \"because\", \"as\", \"until\", \"while\", \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \"against\", \"between\", \"into\", \"through\", \"during\", \"before\", \"after\", \"above\", \"below\", \"to\", \"from\", \"up\", \"down\", \"in\", \"out\", \"on\", \"off\", \"over\", \"under\", \"again\", \"further\", \"then\", \"once\", \"here\", \"there\", \"when\", \"where\", \"why\", \"how\", \"all\", \"any\", \"both\", \"each\", \"few\", \"more\", \"most\", \"other\", \"some\", \"such\", \"no\", \"nor\", \"not\", \"only\", \"own\", \"same\", \"so\", \"than\", \"too\", \"very\", \"s\", \"t\", \"can\", \"will\", \"just\", \"don\", \"should\", \"now\"]\n",
    "\n",
    "additional_stopwords = ['university', 'department']\n",
    "\n",
    "stopwords.extend(additional_stopwords)\n",
    "\n",
    "stopwords = sorted(stopwords)\n",
    "\n",
    "\n",
    "def custom_affiliation_tester(affiliation_1, affiliation_2):\n",
    "    if affiliation_1 == '' or affiliation_2 == '':\n",
    "        return 0\n",
    "    else:\n",
    "        affiliation_1 = affiliation_1.lower()\n",
    "        affiliation_2 = affiliation_2.lower()\n",
    "\n",
    "        processed = []\n",
    "\n",
    "        for aff in [affiliation_1,  affiliation_2]:\n",
    "            yo = copy.deepcopy(aff)\n",
    "            for punc in punctuation:\n",
    "                if punc in aff:\n",
    "                    yo = yo.replace(punc, \"\")\n",
    "            processed.append(yo)\n",
    "\n",
    "        affiliation_1 = processed[0]\n",
    "        affiliation_2 = processed[1]\n",
    "\n",
    "\n",
    "\n",
    "        affiliation_1 = sorted(list(set(affiliation_1.split())))\n",
    "        affiliation_2 = sorted(list(set(affiliation_2.split())))\n",
    "\n",
    "\n",
    "        processed2 = []\n",
    "\n",
    "        for aff in [affiliation_1,  affiliation_2]:\n",
    "            yo = copy.deepcopy(aff)\n",
    "            for word in aff:\n",
    "                for stopword in stopwords:\n",
    "                    if stopword == word:\n",
    "                        yo.remove(word)\n",
    "            processed2.append(yo)\n",
    "\n",
    "        affiliation_1 = processed2[0]\n",
    "        affiliation_2 = processed2[1]\n",
    "\n",
    "        affiliation_1 = sorted(list(set(affiliation_1)))\n",
    "        affiliation_2 = sorted(list(set(affiliation_2)))\n",
    "\n",
    "\n",
    "        if len(affiliation_1) <= len(affiliation_2):\n",
    "            shorter_affiliation = affiliation_1\n",
    "            longer_affiliation = affiliation_2\n",
    "        else:\n",
    "            shorter_affiliation = affiliation_2\n",
    "            longer_affiliation = affiliation_1\n",
    "\n",
    "        count = 0\n",
    "        for word in shorter_affiliation:\n",
    "            for word2 in longer_affiliation:\n",
    "                if word == word2:\n",
    "                    count += 1\n",
    "\n",
    "        final_count = count\n",
    "        \n",
    "        if len(shorter_affiliation) != 0:\n",
    "            final_percentage = count/len(shorter_affiliation)*100\n",
    "        else:\n",
    "            final_percentage = 0\n",
    "        \n",
    "\n",
    "        return final_percentage\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting variables for indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ID_index = 0\n",
    "first_name_index = 1\n",
    "middle_name_index = 2\n",
    "last_name_index = 3\n",
    "full_name_index = 4\n",
    "affiliation_index = 5\n",
    "activity_index = 6\n",
    "name_uniqueness_index = 7\n",
    "first_name_uniqueness_index = 8\n",
    "new_id_index = 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limit how many names are used in the test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = 0\n",
    "stretch = 5000\n",
    "limit = start + stretch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Source 1 - PubMed extraction\n",
    "\n",
    "#### Run this cell if data to be used is from the above PubMed extraction. Ignore the 'Data Source 2 - External CSV' cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_extraction = [x[:7] for x in author_extraction]\n",
    "if len(author_extraction) < limit:\n",
    "    names4 = author_extraction\n",
    "else:\n",
    "    names4 = author_extraction[start:limit]\n",
    "    \n",
    "    \n",
    "publication_dict = extraction_publication_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Source 2 - External CSVs\n",
    "\n",
    "#### Run this cell if data to be used is from the 'Names.csv' and 'Publication Titles.csv' files. Ignore the 'Data Source 1 - PubMed extraction' cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Names.csv', encoding = \"ISO-8859-1\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    names3 = list(reader)\n",
    "    \n",
    "if len(names3) < limit:\n",
    "    names4 = names3\n",
    "else:\n",
    "    names4 = names3[start:limit]\n",
    "    \n",
    "    \n",
    "with open('Publication Titles.csv', encoding = \"utf-8-sig\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    publication_list = list(reader)\n",
    "    \n",
    "    \n",
    "publication_dict = {}\n",
    "for x in publication_list:\n",
    "    publication_dict[x[0]] = x[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standardisation of author records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12:59:03.320473\n",
      "Finished preprocessing... 12:59:14.625448\n",
      "Data set up... 12:59:14.637414\n",
      "Vectorizer fit... 12:59:14.769063\n",
      "Cosine similarities calculated... 12:59:15.663816\n",
      "Calculating nonzeros... 12:59:15.802438\n",
      "Cosine list made... 12:59:15.904166\n",
      "Affiliation vectorizer fit... 12:59:16.638535\n",
      "Affiliation cosine similarities calculated... 12:59:17.732133\n",
      "Calculating affiliation nonzeroes... 12:59:17.852811\n",
      "Affiliation cosine list made... 12:59:17.876748\n",
      "Affiliation vectorizer 2 fit... 12:59:25.487163\n",
      "Affiliation cosine similarities 2 calculated... 12:59:44.605721\n",
      "Calculating affiliation 2 nonzeroes... 12:59:44.717421\n",
      "Affiliation cosine list 2 made... 12:59:44.743351\n",
      "\n",
      "Done 500... 12:59:47.139947\n",
      "Done 1000... 12:59:50.300506\n",
      "Done 1500... 12:59:53.194688\n",
      "Done 2000... 12:59:55.243243\n",
      "Done 2500... 12:59:57.460284\n",
      "Done 3000... 13:00:00.344602\n",
      "Done 3500... 13:00:02.636447\n",
      "Done 4000... 13:00:05.160729\n",
      "Done 4500... 13:00:07.078571\n",
      "Done 5000... 13:00:09.136773\n",
      "\n",
      "923 matches from 5000 records. 244848 comparisons \n",
      "\n",
      "Accuracy of 0.9924606286349081 \n",
      "\n",
      "True positives = 0. True positive percentage = None \n",
      "\n",
      "False positives = 1846. False positive percentage = 0.007539371365091812 \n",
      "\n",
      "False negatives = 0. False negative percentage = None \n",
      "\n",
      "Wall time: 1min 6s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "#Preprocess names - set to lowercase and remove special characters and accents etc.\n",
    "for x in names4:\n",
    "    x[full_name_index] = unidecode(x[full_name_index].lower())\n",
    "    x[first_name_index] = unidecode(x[first_name_index].lower())\n",
    "    x[middle_name_index] = unidecode(x[middle_name_index].lower())\n",
    "    x[last_name_index] = unidecode(x[last_name_index].lower())\n",
    "\n",
    "#Set name uniqueness\n",
    "first_last_names4 = [(x[first_name_index], x[last_name_index]) for x in names4 if len(x[first_name_index]) > 1]\n",
    "first_last_names4 = set(first_last_names4)\n",
    "first_last_names4 = list(first_last_names4)\n",
    "\n",
    "\n",
    "for x in names4:\n",
    "    if x[last_name_index] in [y[1] for y in first_last_names4]:\n",
    "        x.append([y[1] for y in first_last_names4].count(x[last_name_index])** -1)\n",
    "    else:\n",
    "        x.append(1.0)\n",
    "    \n",
    "for x in names4:\n",
    "    if x[first_name_index] in [y[0] for y in first_last_names4]:\n",
    "        x.append([y[0] for y in first_last_names4].count(x[first_name_index])** -1)\n",
    "    else:\n",
    "        x.append(1.0)\n",
    "    \n",
    "print('Finished preprocessing...', datetime.datetime.now().time())\n",
    "#Populate dictionary    \n",
    "IDs = list(range(len(names4)))\n",
    "tuple_records = list(zip(IDs, names4))\n",
    "\n",
    "print('Data set up...', datetime.datetime.now().time())\n",
    "\n",
    "#######Cosine similarity calculations etc\n",
    "all_names = [x[1][full_name_index] for x in tuple_records]\n",
    "all_names_labels = [(y, x[1][full_name_index]) for (y, x) in enumerate(tuple_records)]\n",
    "\n",
    "yup = TfidfVectorizer(analyzer='char', ngram_range=(2,2))\n",
    "\n",
    "yup.fit(all_names)\n",
    "\n",
    "print('Vectorizer fit...', datetime.datetime.now().time())\n",
    "\n",
    "cosine_similarities = cosine_similarity(yup.fit_transform(all_names))\n",
    "\n",
    "print('Cosine similarities calculated...', datetime.datetime.now().time())\n",
    "\n",
    "\n",
    "cos_threshold = 0.3\n",
    "\n",
    "\n",
    "yikes = np.nonzero(cosine_similarities>cos_threshold)\n",
    "\n",
    "print('Calculating nonzeros...', datetime.datetime.now().time())\n",
    "\n",
    "cosine_list = [(i, j) for i, j in zip(yikes[0], yikes[1])]\n",
    "\n",
    "print('Cosine list made...', datetime.datetime.now().time())\n",
    "\n",
    "\n",
    "#####Setting affiliation cosine similarities\n",
    "\n",
    "all_affiliations = [x[1][affiliation_index] for x in tuple_records]\n",
    "\n",
    "\n",
    "affiliation_vectorizer = TfidfVectorizer(analyzer='word', ngram_range=(1, 3), stop_words='english')\n",
    "affiliation_vectorizer.fit(all_affiliations)\n",
    "print('Affiliation vectorizer fit...', datetime.datetime.now().time())\n",
    "\n",
    "affiliation_cosine_similarities = cosine_similarity(affiliation_vectorizer.fit_transform(all_affiliations))\n",
    "print('Affiliation cosine similarities calculated...', datetime.datetime.now().time())\n",
    "\n",
    "affiliation_nonzeroes = np.nonzero(affiliation_cosine_similarities>0.7)\n",
    "print('Calculating affiliation nonzeroes...', datetime.datetime.now().time())\n",
    "\n",
    "affiliation_cosine_list = [(i, j) for i, j in zip(affiliation_nonzeroes[0], affiliation_nonzeroes[1])]\n",
    "print('Affiliation cosine list made...', datetime.datetime.now().time())\n",
    "\n",
    "### And again for a second vectorizer\n",
    "\n",
    "affiliation_vectorizer2 = TfidfVectorizer(analyzer='char', ngram_range=(2, 10))\n",
    "affiliation_vectorizer2.fit(all_affiliations)\n",
    "print('Affiliation vectorizer 2 fit...', datetime.datetime.now().time())\n",
    "\n",
    "affiliation_cosine_similarities2 = cosine_similarity(affiliation_vectorizer2.fit_transform(all_affiliations))\n",
    "print('Affiliation cosine similarities 2 calculated...', datetime.datetime.now().time())\n",
    "\n",
    "affiliation_nonzeroes2 = np.nonzero(affiliation_cosine_similarities2>0.7)\n",
    "print('Calculating affiliation 2 nonzeroes...', datetime.datetime.now().time())\n",
    "\n",
    "affiliation_cosine_list2 = [(i, j) for i, j in zip(affiliation_nonzeroes2[0], affiliation_nonzeroes2[1])]\n",
    "print('Affiliation cosine list 2 made...', datetime.datetime.now().time())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "######Below are all the tests etc\n",
    "\n",
    "#Thresholds and variables\n",
    "matches = []\n",
    "match = []\n",
    "predicted = {}\n",
    "actual = {}\n",
    "name_match_record = {}\n",
    "\n",
    "\n",
    "affiliation_threshold_1 = 0.1\n",
    "affiliation_threshold_2 = 0.25\n",
    "\n",
    "print()\n",
    "\n",
    "\n",
    "################# The tests after filtering for cosine simialrities above the threshold####\n",
    "progress = 0\n",
    "current_i = None\n",
    "for record in cosine_list:\n",
    "\n",
    "    i = record[0]\n",
    "    j = record[1]\n",
    "    if i != current_i:\n",
    "        progress += 1\n",
    "        if (progress/500) == int(progress/500):\n",
    "            print(f'Done {progress}...', datetime.datetime.now().time())\n",
    "    current_i = i\n",
    "    this_match = False\n",
    "    reason = None\n",
    "    name_match_reason = None\n",
    "    affiliation_score = None\n",
    "    affiliation_score_2 = None\n",
    "    both_affiliations_present = False\n",
    "    name_score = None\n",
    "    space_dash = True\n",
    "    first_name_score = None\n",
    "    last_name_score = None\n",
    "    network_reason = None\n",
    "    single_letter_initials = True\n",
    "    name_match_count = 0\n",
    "    aff_test = False\n",
    "    additional_name_1 = None\n",
    "    additional_name_2 = None\n",
    "    additional_name_score = None\n",
    "    last_name_used = False\n",
    "    if (i != j\n",
    "        and (\n",
    "            (tuple_records[i][1][activity_index] != tuple_records[j][1][activity_index])\n",
    "            or (tuple_records[i][1][activity_index] == tuple_records[j][1][activity_index] and (tuple_records[i][1][first_name_index], tuple_records[i][1][last_name_index]) == (tuple_records[j][1][first_name_index], tuple_records[j][1][last_name_index]))\n",
    "        )):\n",
    "        predicted.update({(i, j): [False, reason]})\n",
    "        name_match_record.update({(i, j): [False, name_match_reason]})\n",
    "        if tuple_records[i][1][ID_index] == tuple_records[j][1][ID_index]:\n",
    "            actual.update({(i, j): True})\n",
    "            actual_value = True\n",
    "        else:\n",
    "            actual.update({(i, j): False})\n",
    "            actual_value = False\n",
    "\n",
    "        ### Preliminary tests to set some variables used in future tests\n",
    "        if tuple_records[i][1][affiliation_index] != '' and tuple_records[j][1][affiliation_index] != '':\n",
    "            both_affiliations_present = True\n",
    "\n",
    "        if len(tuple_records[i][1][first_name_index]) > 1 and len(tuple_records[j][1][first_name_index]) > 1:\n",
    "            single_letter_initials = False\n",
    "\n",
    "        if additional_name_1 == tuple_records[i][1][last_name_index] and additional_name_2 == tuple_records[j][1][last_name_index]:\n",
    "            if (' ' not in additional_name_1\n",
    "                and '-' not in additional_name_1\n",
    "                and ' ' not in additional_name_2\n",
    "                and '-' not in additional_name_2):\n",
    "                space_dash = False\n",
    "        \n",
    "        #Setting affiliation cosine similarity score        \n",
    "        affiliation_score_1 = affiliation_cosine_similarities[i, j]\n",
    "        affiliation_score_2 = affiliation_cosine_similarities2[i, j]\n",
    "\n",
    "\n",
    "        #Exact Match\n",
    "        if this_match == False and single_letter_initials == False:\n",
    "            if tuple_records[i][1][full_name_index] == tuple_records[j][1][full_name_index]:\n",
    "                this_match = True\n",
    "                reason = 'exact_match'\n",
    "                network_reason = 'exact_match'\n",
    "\n",
    "        #First name and Last Name match but different middle name (middle names either completely different or one is missing)\n",
    "        if this_match == False and single_letter_initials == False and reason == None and aff_test == False:\n",
    "            if (tuple_records[i][1][first_name_index], tuple_records[i][1][last_name_index]) == (tuple_records[j][1][first_name_index], tuple_records[j][1][last_name_index]):\n",
    "                if tuple_records[i][1][middle_name_index] == '' or tuple_records[j][1][middle_name_index] == '':\n",
    "                    if tuple_records[i][1][name_uniqueness_index] <= 1/5 or tuple_records[j][1][name_uniqueness_index] <= 1/5:\n",
    "                        aff_test = True #Because names are common\n",
    "                        reason = 'FN_LN_match'\n",
    "                        network_reason = 'FN_LN_match'                        \n",
    "                    else:\n",
    "                        this_match = True\n",
    "                        reason = 'FN_LN_match'\n",
    "                        network_reason = 'FN_LN_match'\n",
    "                else: #both have a middle name\n",
    "                    if len(tuple_records[i][1][middle_name_index]) > 1 and len(tuple_records[j][1][middle_name_index]) > 1:\n",
    "                        aff_test = True #because middle names are different\n",
    "                        reason = 'FN_LN_match'\n",
    "                        network_reason = 'FN_LN_match'\n",
    "                    else:#only one has multiple letters\n",
    "                        if tuple_records[i][1][middle_name_index][0] == tuple_records[j][1][middle_name_index][0]:\n",
    "                            if tuple_records[i][1][name_uniqueness_index] <= 1/5 or tuple_records[j][1][name_uniqueness_index] <= 1/5:\n",
    "                                aff_test = True #because middle names are different\n",
    "                                reason = 'FN_LN_match'\n",
    "                                network_reason = 'FN_LN_match'  \n",
    "                            else:\n",
    "                                this_match = True #only one has multiple letters but first letter of middle name is the same\n",
    "                                reason = 'FN_LN_match'\n",
    "                                network_reason = 'FN_LN_match'\n",
    "                             \n",
    "                                \n",
    "\n",
    "        #First and last names match but reversed\n",
    "        if this_match == False and single_letter_initials == False and reason == None and aff_test == False:\n",
    "            if tuple_records[i][1][name_uniqueness_index] <= 1/5 or tuple_records[j][1][name_uniqueness_index] <= 1/5:\n",
    "                if ((tuple_records[i][1][first_name_index], tuple_records[i][1][last_name_index]) == (tuple_records[j][1][last_name_index], tuple_records[j][1][first_name_index])):\n",
    "                    aff_test = True\n",
    "                    reason = 'FN_LN_reversed'\n",
    "                    network_reason = 'FN_LN_reversed'                \n",
    "            else:\n",
    "                if ((tuple_records[i][1][first_name_index], tuple_records[i][1][last_name_index]) == (tuple_records[j][1][last_name_index], tuple_records[j][1][first_name_index])):\n",
    "                    this_match = True\n",
    "                    reason = 'FN_LN_reversed'\n",
    "                    network_reason = 'FN_LN_reversed'\n",
    "\n",
    "\n",
    "        #############Fuzzy check now#############\n",
    "        if this_match == False and reason == None and aff_test == False:\n",
    "            name_score = fuzz.token_sort_ratio(tuple_records[i][1][full_name_index], tuple_records[j][1][full_name_index])\n",
    "            first_name_score = fuzz.partial_ratio(tuple_records[i][1][first_name_index], tuple_records[j][1][first_name_index])\n",
    "            last_name_score = fuzz.partial_ratio(tuple_records[i][1][last_name_index],tuple_records[j][1][last_name_index])\n",
    "            first_name_score_whole = fuzz.ratio(tuple_records[i][1][first_name_index], tuple_records[j][1][first_name_index])\n",
    "            last_name_score_whole = fuzz.ratio(tuple_records[i][1][last_name_index],tuple_records[j][1][last_name_index])\n",
    "\n",
    "\n",
    "\n",
    "        #Names jumbled\n",
    "        if this_match == False and reason == None and aff_test == False:\n",
    "            if name_score == 100:\n",
    "                this_match = True\n",
    "                reason = 'names_jumbled'\n",
    "                network_reason = 'names_jumbled'\n",
    "\n",
    "\n",
    "        #Fuzzy\n",
    "        if this_match == False and single_letter_initials == False and reason == None and aff_test == False:\n",
    "            if name_score >= 95 or (first_name_score_whole >= 85 and last_name_score_whole >= 85):\n",
    "                aff_test = True\n",
    "                reason = 'fuzzy_level_1'\n",
    "                network_reason = 'fuzzy_level_1'\n",
    "            elif name_score >= 85:\n",
    "                if (first_name_score + last_name_score >= 170\n",
    "                    and first_name_score >= 70\n",
    "                    and last_name_score >= 70):\n",
    "                    reason = 'fuzzy_level_2'\n",
    "                    network_reason = 'fuzzy_level_2'\n",
    "            elif name_score >= 70:\n",
    "                if (first_name_score + last_name_score >= 150\n",
    "                    and first_name_score >= 50\n",
    "                    and last_name_score >= 50):\n",
    "                    reason = 'fuzzy_level_3'\n",
    "\n",
    "\n",
    "\n",
    "        #Initials\n",
    "        if this_match == False and reason == None and aff_test == False:\n",
    "            if initial_tester(tuple_records[i][1][first_name_index:last_name_index + 1],\n",
    "                              tuple_records[j][1][first_name_index:last_name_index + 1]) == True:\n",
    "                if tuple_records[i][1][name_uniqueness_index] >= 0.5:\n",
    "                    this_match = True\n",
    "                    reason = 'initials'\n",
    "                    network_reason = 'initials'\n",
    "                else:\n",
    "                    aff_test = True\n",
    "                    network_reason = 'initials'\n",
    "                    reason = 'initials'\n",
    "\n",
    "        #Hyphen in name\n",
    "        if this_match == False and reason == None and aff_test == False:\n",
    "            if hyphen_tester(tuple_records[i][1][first_name_index:last_name_index + 1],\n",
    "                              tuple_records[j][1][first_name_index:last_name_index + 1]) == True:\n",
    "                aff_test = True\n",
    "                network_reason = 'hyphen'\n",
    "                reason = 'hyphen'\n",
    "\n",
    "\n",
    "        #Some names appear\n",
    "        name_match_count = 0\n",
    "        additional_name_score = None\n",
    "        if this_match == False and single_letter_initials == False and aff_test == False:\n",
    "            for name in tuple_records[i][1][first_name_index:last_name_index + 1]:\n",
    "                for name2 in tuple_records[j][1][first_name_index:last_name_index + 1]:\n",
    "                    if name == name2 and name != '' and len(name) > 1:\n",
    "                        name_match_count += 1 \n",
    "            if name_match_count >= 2:\n",
    "                reason = 'some_names'\n",
    "                network_reason = 'some_names'\n",
    "                for add_name in tuple_records[i][1][first_name_index:last_name_index + 1]:\n",
    "                    if add_name not in tuple_records[j][1][first_name_index:last_name_index + 1]:\n",
    "                        additional_name_1 = add_name\n",
    "                for add_name2 in tuple_records[j][1][first_name_index:last_name_index + 1]:\n",
    "                    if add_name2 not in tuple_records[i][1][first_name_index:last_name_index + 1]:\n",
    "                        additional_name_2 = add_name2\n",
    "\n",
    "                additional_name_score = fuzz.partial_ratio(additional_name_1, additional_name_2)\n",
    "\n",
    "                if additional_name_score == 100 and both_affiliations_present == False:\n",
    "                    this_match = True\n",
    "                elif additional_name_score > 80:\n",
    "                    aff_test = True\n",
    "                elif additional_name_1 == '' or additional_name_2 == '':\n",
    "                    aff_test = True\n",
    "\n",
    "\n",
    "\n",
    "        #Names contained\n",
    "        if this_match == False and aff_test == False:\n",
    "            if space_dash == True:\n",
    "                if (first_name_score == 100 and last_name_score == 100\n",
    "                    and len(tuple_records[i][1][first_name_index]) >3\n",
    "                    and len(tuple_records[j][1][first_name_index]) >3\n",
    "                    and len(tuple_records[i][1][last_name_index])  >3\n",
    "                    and len(tuple_records[j][1][last_name_index])  >3):\n",
    "                    aff_test = True\n",
    "                    reason = 'names_contained'\n",
    "                    network_reason = 'names_contained'\n",
    "            else:\n",
    "                if (first_name_score == 100 and tuple_records[i][1][last_name_index] == tuple_records[j][1][last_name_index]\n",
    "                    and len(tuple_records[i][1][first_name_index]) >3\n",
    "                    and len(tuple_records[j][1][first_name_index]) >3\n",
    "                    and len(tuple_records[i][1][last_name_index])  >3\n",
    "                    and len(tuple_records[j][1][last_name_index])  >3):\n",
    "                    aff_test = True\n",
    "                    reason = 'names_contained'\n",
    "                    network_reason = 'names_contained'\n",
    "                \n",
    "\n",
    "\n",
    "\n",
    "        #Affiliation tests and final decisions now\n",
    "        if this_match == False and aff_test == True and both_affiliations_present == True:\n",
    "\n",
    "\n",
    "\n",
    "            if reason == 'FN_LN_match':\n",
    "                if affiliation_score_1 >= affiliation_threshold_1 or affiliation_score_2 >= affiliation_threshold_2: this_match = True\n",
    "                    \n",
    "            if reason == 'FN_LN_reversed':\n",
    "                if affiliation_score_1 >= affiliation_threshold_1 or affiliation_score_2 >= affiliation_threshold_2: this_match = True\n",
    "\n",
    "            if reason == 'fuzzy_level_1':\n",
    "                if affiliation_score_1 >= affiliation_threshold_1 or affiliation_score_2 >= affiliation_threshold_2: this_match = True\n",
    "\n",
    "            elif reason == 'initials':\n",
    "                if affiliation_score_1 >= affiliation_threshold_1 or affiliation_score_2 >= affiliation_threshold_2: this_match = True\n",
    "\n",
    "            elif reason == 'hyphen':\n",
    "                if affiliation_score_1 >= affiliation_threshold_1 or affiliation_score_2 >= affiliation_threshold_2: this_match = True\n",
    "\n",
    "            elif reason == 'one_name':\n",
    "                if affiliation_score_1 >= affiliation_threshold_1 or affiliation_score_2 >= affiliation_threshold_2: this_match = True\n",
    "\n",
    "            elif reason == 'some_names':\n",
    "                if affiliation_score_1 >= affiliation_threshold_1 or affiliation_score_2 >= affiliation_threshold_2: this_match = True\n",
    "\n",
    "            elif reason == 'names_contained':\n",
    "                if affiliation_score_1 >= affiliation_threshold_1 or affiliation_score_2 >= affiliation_threshold_2: this_match = True\n",
    "                    \n",
    "        \n",
    "        if (this_match == True\n",
    "            and\n",
    "            (reason == 'initials' or (reason == 'exact_match' and tuple_records[i][1][name_uniqueness_index] <1/2))\n",
    "            and\n",
    "            affiliation_score_1 <= 0.1\n",
    "            and\n",
    "            affiliation_score_2 <= 0.1\n",
    "            and\n",
    "            both_affiliations_present == True):\n",
    "            affiliation_score_3 = custom_affiliation_tester(tuple_records[i][1][affiliation_index], \n",
    "                                                           tuple_records[j][1][affiliation_index])\n",
    "            if reason == 'initals':\n",
    "                if affiliation_score_3 < 16.6:\n",
    "                    this_match = False\n",
    "            elif reason == 'exact_match':\n",
    "                this_match = False\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        if this_match == True:\n",
    "            predicted.update({(i, j): [True, reason]})\n",
    "            match = [tuple_records[i][0], tuple_records[j][0]]\n",
    "            match.sort()\n",
    "            match.append(name_score)\n",
    "            match.append(affiliation_score)\n",
    "            match.append(actual_value)\n",
    "            match.append(first_name_score)\n",
    "            match.append(last_name_score)\n",
    "            if matches == [] or match not in matches:\n",
    "                matches.append(match)\n",
    "        else:\n",
    "            predicted.update({(i, j): [False, reason]})\n",
    "\n",
    "        if network_reason != None:\n",
    "            name_match_record.update({(i, j): [True, network_reason]})\n",
    "                \n",
    "\n",
    "            \n",
    "            \n",
    "#_________________End of match assessment - the below is just for recording the results.__________________________________\n",
    "\n",
    "correct_count = 0\n",
    "for key in actual:\n",
    "    if actual[key] == predicted[key][0]:\n",
    "        correct_count = correct_count + 1\n",
    "        \n",
    "overall_accuracy = correct_count / len(actual)\n",
    "   \n",
    "    \n",
    "true_positives = 0\n",
    "for key in actual:\n",
    "    if actual[key] == predicted[key][0] and actual[key] == True:\n",
    "        true_positives = true_positives + 1\n",
    "        \n",
    "true_total = sum(value == True for value in actual.values())\n",
    "if true_total > 0:\n",
    "    true_positive_percentage = true_positives / true_total\n",
    "else:\n",
    "    true_positive_percentage = None\n",
    "\n",
    "\n",
    "false_positives = 0\n",
    "for key in actual:\n",
    "    if actual[key] != predicted[key][0] and actual[key] == False:\n",
    "        false_positives = false_positives + 1     \n",
    "\n",
    "false_total = sum(value == False for value in actual.values())\n",
    "if false_total > 0:\n",
    "    false_positive_percentage = false_positives / false_total\n",
    "else:\n",
    "    false_positive_percentage = None\n",
    "    \n",
    "    \n",
    "\n",
    "false_negatives = 0\n",
    "for key in actual:\n",
    "    if actual[key] != predicted[key][0] and actual[key] == True:\n",
    "        false_negatives = false_negatives + 1     \n",
    "\n",
    "if true_total > 0:\n",
    "    false_negative_percentage = false_negatives / true_total\n",
    "else:\n",
    "    false_negative_percentage = None\n",
    "\n",
    "\n",
    "\n",
    "print()\n",
    "print('{} matches from {} records. {} comparisons'.format(len(matches), len(names4), len(actual)), '\\n')\n",
    "\n",
    "# print('Accuracy of {}'.format(overall_accuracy), '\\n')\n",
    "# print('True positives = {}. True positive percentage = {}'.format(true_positives, true_positive_percentage), '\\n')\n",
    "# print('False positives = {}. False positive percentage = {}'.format(false_positives, false_positive_percentage), '\\n')\n",
    "# print('False negatives = {}. False negative percentage = {}'.format(false_negatives, false_negative_percentage), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardisation results export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "false_positives_export_list = (\n",
    "                                [['ID 1', 'First Name 1', 'Middle Name 1', 'Last Name 1', 'Affiliation 1', 'Name Uniqueness 1',\n",
    "                               'ID 2', 'First Name 2', 'Middle Name 2', 'Last Name 2', 'Affiliation 2', 'Name Uniqueness 2',\n",
    "                               'Reason', 'Affiliation Cosine Similarity', 'Match Status']]\n",
    "                              )\n",
    "for key in actual:\n",
    "    if (actual[key] != predicted[key][0]\n",
    "        and actual[key] == False):\n",
    "        false_positives_export_list.append([tuple_records[key[0]][1][0],\n",
    "                                            tuple_records[key[0]][1][1],\n",
    "                                            tuple_records[key[0]][1][2],\n",
    "                                            tuple_records[key[0]][1][3],\n",
    "                                            tuple_records[key[0]][1][5],\n",
    "                                            tuple_records[key[0]][1][7],\n",
    "                                            tuple_records[key[1]][1][0],\n",
    "                                            tuple_records[key[1]][1][1],\n",
    "                                            tuple_records[key[1]][1][2],\n",
    "                                            tuple_records[key[1]][1][3],\n",
    "                                            tuple_records[key[1]][1][5],\n",
    "                                            tuple_records[key[1]][1][7],\n",
    "                                            predicted[key][1],\n",
    "                                            affiliation_cosine_similarities[key[0], key[1]],\n",
    "                                            'Match'\n",
    "                                           ])\n",
    "\n",
    "\n",
    "for key in actual:\n",
    "    if (actual[key] == predicted[key][0]\n",
    "        and actual[key] == False\n",
    "        and predicted[key][1] != None):\n",
    "        false_positives_export_list.append([tuple_records[key[0]][1][0],\n",
    "                                            tuple_records[key[0]][1][1],\n",
    "                                            tuple_records[key[0]][1][2],\n",
    "                                            tuple_records[key[0]][1][3],\n",
    "                                            tuple_records[key[0]][1][5],\n",
    "                                            tuple_records[key[0]][1][7],\n",
    "                                            tuple_records[key[1]][1][0],\n",
    "                                            tuple_records[key[1]][1][1],\n",
    "                                            tuple_records[key[1]][1][2],\n",
    "                                            tuple_records[key[1]][1][3],\n",
    "                                            tuple_records[key[1]][1][5],\n",
    "                                            tuple_records[key[1]][1][7],\n",
    "                                            predicted[key][1],\n",
    "                                            affiliation_cosine_similarities[key[0], key[1]],\n",
    "                                            'Near Match'\n",
    "                                           ])\n",
    "\n",
    "\n",
    "with open('Matching Export.csv','w', newline='', encoding = \"UTF-8\") as dunno:\n",
    "    writer = csv.writer(dunno)\n",
    "    writer.writerows(false_positives_export_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unification of Author IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 5.73 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import copy\n",
    "person_records = copy.deepcopy(tuple_records)\n",
    "\n",
    "prefix = 'test_'\n",
    "\n",
    "IDs_present = True\n",
    "\n",
    "if IDs_present == False:\n",
    "    for i in person_records:\n",
    "        i[1][0] = prefix + i[1][0] \n",
    "\n",
    "\n",
    "for key in predicted:\n",
    "    if predicted[key][0] == True:\n",
    "        if len(person_records[key[0]][1]) == 9:\n",
    "            person_records[key[0]][1].append(None)\n",
    "        if len(person_records[key[1]][1]) == 9:\n",
    "            person_records[key[1]][1].append(None)\n",
    "\n",
    "for key in predicted:\n",
    "    if predicted[key][0] == True:\n",
    "        if person_records[key[0]][1][9] == None: #or person_records[key[0]][1][9] == person_records[key[0]][1][0]:\n",
    "            \n",
    "            if person_records[key[1]][1][9] == None:            \n",
    "                person_records[key[0]][1][9] = person_records[key[0]][1][0]\n",
    "                person_records[key[1]][1][9] = person_records[key[0]][1][0]\n",
    "            else:\n",
    "                person_records[key[0]][1][9] = person_records[key[1]][1][9]\n",
    "        elif person_records[key[0]][1][9] != None and person_records[key[1]][1][9] != None:\n",
    "            person_records[key[0]][1][9] = person_records[key[1]][1][9]\n",
    "            for person in person_records:\n",
    "                if len(person[1]) == 10 and person[1][9] == person_records[key[0]][1][9]:\n",
    "                    person[1][9] = person_records[key[0]][1][9]\n",
    "        else:\n",
    "            person_records[key[1]][1][9] = person_records[key[0]][1][9]\n",
    "            \n",
    "for x in person_records:\n",
    "    if len(x[1]) == 9:\n",
    "        x[1].append(x[1][0])\n",
    "            \n",
    "person_records2 = copy.deepcopy(person_records)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All IDs in the dataset and final IDs export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_list = [['Original ID', 'First Name', 'Middle Name', 'Last Name', 'Full Name', 'Affiliation', 'Activity ID', 'Name Uniqueness', 'First Name Uniqueness', 'New ID']] + [x[1] for x in person_records]\n",
    "\n",
    "with open('Names_results_Stage_1_visit.csv','w', newline='', encoding = \"ISO-8859-1\") as dunno:\n",
    "    writer = csv.writer(dunno)\n",
    "    writer.writerows(results_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attributing publications to Author IDs after unifying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "activities_dict = {}\n",
    "for i in person_records:\n",
    "    current_activity = i[1][6]\n",
    "    if current_activity not in activities_dict:\n",
    "        activities_dict.update({current_activity: []})\n",
    "        \n",
    "    activities_dict[current_activity].append(i[1][9])\n",
    "\n",
    "        \n",
    "\n",
    "for x in activities_dict:\n",
    "    activities_dict[x] = list(set(activities_dict[x]))\n",
    "\n",
    "people = {}\n",
    "\n",
    "for i in person_records:\n",
    "    current_ID = i[1][9]\n",
    "    if current_ID not in people:\n",
    "        people.update({current_ID: {'numbers': [],\n",
    "                                    'first_name': i[1][1],\n",
    "                                    'middle_name': i[1][2],\n",
    "                                    'last_name': i[1][3],\n",
    "                                    'full_name': i[1][4],\n",
    "                                    'original_IDs': [],\n",
    "                                    'activities': [],\n",
    "                                    'collaborators': []}\n",
    "                      })\n",
    "        \n",
    "    people[current_ID]['numbers'].append(i[0])\n",
    "    people[current_ID]['original_IDs'].append(i[1][0])\n",
    "    people[current_ID]['activities'].append(i[1][6])\n",
    "    people[current_ID]['collaborators'].extend(activities_dict[i[1][6]])\n",
    "        \n",
    "    \n",
    "for x in people:\n",
    "    people[x]['collaborators'] = list(set(people[x]['collaborators']))\n",
    "    people[x]['collaborators'].remove(x)    \n",
    "    \n",
    "title_list = []\n",
    "person_list = []\n",
    "for person in people:\n",
    "    title_list.append(people[person]['activities'])\n",
    "    person_list.append(person)\n",
    "    \n",
    "\n",
    "final_title_list = []\n",
    "for person_articles in title_list:\n",
    "    building = []\n",
    "    for article in person_articles:\n",
    "        for x in publication_dict:\n",
    "            if article == x:\n",
    "                building.append(publication_dict[x])\n",
    "    if building == []:\n",
    "        final_title_list.append('No Title')\n",
    "    else:\n",
    "        final_title_list.append(' '.join(building))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identifying Similar Authors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting parameters for author cosine simiarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collab_threshold = 0 #Set to 0 if only cosine simiarities between authors collaborating on 0% of publications should be found\n",
    "top_number = 5 #Number of 'closest' authors to be included in export\n",
    "similarity_minimum = 0.2 #Minimum cosine similarity to be included in export"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting cosine similarities for author corpi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title vectorizer fit... 20:00:32.583286\n",
      "Title cosine similarities calculated... 20:00:33.097641\n",
      "Wall time: 771 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#######Cosine similarity calculations etc\n",
    "all_titles = final_title_list\n",
    "\n",
    "\n",
    "title_vectorizer = TfidfVectorizer(analyzer='word', ngram_range=(1,2), stop_words='english' )\n",
    "title_vectorizer.fit(all_titles)\n",
    "print('Title vectorizer fit...', datetime.datetime.now().time())\n",
    "\n",
    "title_cosine_similarities = cosine_similarity(title_vectorizer.fit_transform(all_titles))\n",
    "print('Title cosine similarities calculated...', datetime.datetime.now().time())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating new cosine simiarities after filtering out according to selected collab_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 22s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "collaboration_percentage_array = np.zeros((len(person_list), len(person_list)))\n",
    "for x, y in np.ndenumerate(collaboration_percentage_array):\n",
    "    collaboration_percentage_array[x[0], x[1]] = len(set(people[person_list[x[0]]]['activities'])&set(people[person_list[x[1]]]['activities'])) / len(set(people[person_list[x[0]]]['activities']))* 100\n",
    "    \n",
    "collaboration_threshold_array = np.zeros((len(person_list), len(person_list)))\n",
    "for x in range(len(person_list)):\n",
    "    for y in range(len(person_list)):\n",
    "        if collaboration_percentage_array[x, y] <= collab_threshold:\n",
    "            collaboration_threshold_array[x, y] = 1\n",
    "            \n",
    "filtered_title_cosine_similarities = np.multiply(title_cosine_similarities, collaboration_threshold_array)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the author cosine similarity export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12273"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_number = -top_number\n",
    "similarity_export_list = [['ID', 'Full Name', 'Publications', 'Number of Publications', 'Similar to ID', 'Similar to Full Name', 'Similar to Publications', 'Cosine Similarity', 'Collaboration Percentage']]\n",
    "\n",
    "\n",
    "for index, person in enumerate(person_list):\n",
    "    example = index\n",
    "    minimum_value = np.partition(filtered_title_cosine_similarities[example], top_number)[top_number]\n",
    "    top5 = np.nonzero((filtered_title_cosine_similarities[example] >= minimum_value) & (filtered_title_cosine_similarities[example] > similarity_minimum))\n",
    "    top5_values = filtered_title_cosine_similarities[example][top5]\n",
    "    for top5_index, match_index  in enumerate(top5[0]):\n",
    "\n",
    "        building = []\n",
    "        pubs_building = []\n",
    "\n",
    "        #same each time\n",
    "        building.append(person)\n",
    "        building.append(people[person]['full_name'])\n",
    "        for pub in people[person]['activities']:\n",
    "            pubs_building.append(publication_dict[pub])\n",
    "        pubs_built = '\\n'.join(pubs_building)\n",
    "        building.append(pubs_built)\n",
    "        building.append(len(people[person]['activities']))\n",
    "\n",
    "        #different for each match        \n",
    "        building.append(person_list[match_index])\n",
    "        building.append(people[person_list[match_index]]['full_name'])\n",
    "        pubs_building = []\n",
    "        for pub in people[person_list[match_index]]['activities']:\n",
    "            pubs_building.append(publication_dict[pub])\n",
    "        pubs_built = '\\n'.join(pubs_building)\n",
    "        building.append(pubs_built)\n",
    "        building.append(top5_values[top5_index])\n",
    "        building.append(len(set(people[person]['activities'])&set(people[person_list[match_index]]['activities']))/len(people[person]['activities'])*100)\n",
    "\n",
    "        similarity_export_list.append(building)\n",
    "        \n",
    "len(similarity_export_list)\n",
    "\n",
    "with open('Filtered Similarity Export AML.csv','w', newline='', encoding = \"UTF-8\") as dunno:\n",
    "    writer = csv.writer(dunno)\n",
    "    writer.writerows(similarity_export_list)\n",
    "    \n",
    "# minimum_value = np.partition(filtered_title_cosine_similarities[example], -5)[-5]\n",
    "# minimum_value_indexes = np.nonzero(filtered_title_cosine_similarities[example] >= minimum_value)\n",
    "#cosine_list = [(i, j) for i, j in zip(yikes[0], yikes[1])]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
